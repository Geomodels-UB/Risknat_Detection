{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Cluster classification. University of Barcelona "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies:\n",
    "numpy:https://numpy.org/\n",
    "\n",
    "pandas:https://pandas.pydata.org/\n",
    "\n",
    "matplotlib:https://matplotlib.org/\n",
    "\n",
    "sklearn:https://scikit-learn.org/\n",
    "\n",
    "xgboost:https://xgboost.readthedocs.io/\n",
    "\n",
    "smote_variants: https://smote-variants.readthedocs.io/\n",
    "\n",
    "imblearn.over_sampling:https://imbalanced-learn.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.data_loader import load_data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Models\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier, RandomForestClassifier, VotingClassifier)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = [KNeighborsClassifier, GaussianNB,\n",
    "    DecisionTreeClassifier, AdaBoostClassifier,\n",
    "    RandomForestClassifier, SVC, ExtraTreesClassifier,\n",
    "    LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis, MLPClassifier, xgb.XGBClassifier\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smote variants\n",
    "import smote_variants as sv\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import smote_variants as sv\n",
    "SPIDER = sv.Stefanowski\n",
    "from src.oversampling import SWIM\n",
    "oversamplers = [sv.polynom_fit_SMOTE, sv.ProWSyn,\n",
    "    sv.SMOTE_IPF, sv.Lee, sv.SMOBD, sv.G_SMOTE, sv.LVQ_SMOTE, sv.Assembled_SMOTE,\n",
    "    sv.SMOTE_TomekLinks, SMOTE, ADASYN,\n",
    "    SWIM, SPIDER \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Input X,y Training Option 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path training data file\n",
    "clusters, _ = load_data('DEGOTALLS_E_Leica_Clustering_dec_2020_jun_2018_Training_6.txt')\n",
    "clusters =clusters.replace('None', np.nan).dropna(axis=1)\\\n",
    "    .drop([\n",
    "        'x', 'y', 'z', 'n_points', 'n_order','file_origin', 'file_destination', 'confidence',\n",
    "        'texture_code_origin', 'texture_code_destination'\n",
    "    ], axis=1)\n",
    "\n",
    "#Delete columns with one value\n",
    "for col in clusters.columns:\n",
    "    if len(clusters[col].unique()) == 1:\n",
    "        clusters.drop(col,inplace=True,axis=1)\n",
    "#Binarize\n",
    "clust=clusters\n",
    "clusters['classification'] = (\n",
    "    ((clusters['classification'] == 'Candidate'))\n",
    ")\n",
    "X = clusters.drop('classification', axis=1)\n",
    "X = (X - X.mean()) / X.std()\n",
    "y = clusters['classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Input X,y Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path test data file\n",
    "clusters, _ = load_data('P170_6_DEGOTALLS_E_Cluster_1.txt')\n",
    "clusters =clusters.replace('None', np.nan).dropna(axis=1)\\\n",
    "    .drop([\n",
    "        'x', 'y', 'z', 'n_points', 'n_order','file_origin', 'file_destination', 'confidence',\n",
    "        'texture_code_origin', 'texture_code_destination'\n",
    "    ], axis=1)\n",
    "\n",
    "#Delete columns with one value\n",
    "for col in clusters.columns:\n",
    "    if len(clusters[col].unique()) == 1:\n",
    "        clusters.drop(col,inplace=True,axis=1)\n",
    "#Binarize\n",
    "clusters['classification'] = (\n",
    "    ((clusters['classification'] == 'Unknow'))\n",
    ")\n",
    "\n",
    "X_test = clusters.drop('classification', axis=1)\n",
    "X_test = (X_test - X_test.mean()) / X_test.std()\n",
    "y_test = clusters['classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Cross Validation properties\n",
    "cv_value=5\n",
    "\n",
    "# Model properties\n",
    "def Knn_function(X_train, y_train):\n",
    "    print(\"0\",'Knn')\n",
    "    param_grid = {'n_neighbors': np.arange(1, 17)} \n",
    "    knn_gscv = GridSearchCV(KNeighborsClassifier(), param_grid, cv=cv_value, return_train_score=False)\n",
    "    knn_gscv.fit(X_train, y_train)\n",
    "    y_predknn = knn_gscv.predict(X_test)\n",
    "    queue.put(y_predknn)\n",
    "    print(\"Knn: \", knn_gscv.best_params_)\n",
    "    print(\"Knn: \", knn_gscv.best_score_)\n",
    "\n",
    "def gs_function(X_train, y_train):\n",
    "    print(\"1\",'Gaussian')\n",
    "    param_grid = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "    gs = GridSearchCV(GaussianNB(), param_grid, cv=cv_value, return_train_score=False)\n",
    "    gs.fit(X_train, y_train)\n",
    "    y_predgs = gs.predict(X_test)\n",
    "    queue.put(y_predgs)\n",
    "    print(\"Gaussian: \", gs.best_params_)\n",
    "    print(\"Gaussian: \",gs.best_score_)               \n",
    "\n",
    "def dtc_function(X_train, y_train):\n",
    "    print(\"2\",'DecisionTreeClassifier')\n",
    "    param_grid =  { 'criterion':['gini','entropy'],'max_depth': np.arange(3, 15)}\n",
    "    dtc = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=cv_value, return_train_score=False)\n",
    "    dtc.fit(X_train, y_train)\n",
    "    y_preddtc = dtc.predict(X_test)\n",
    "    queue.put(y_preddtc)\n",
    "    print(\"Decision Tree: \", dtc.best_params_)\n",
    "    print(\"Decision Tree: \", dtc.best_score_)    \n",
    "\n",
    "def abc_function(X_train, y_train):\n",
    "    print(\"3\",'AdaBoostClassifier')\n",
    "    param_grid =  {'n_estimators':np.arange(1, 50),\n",
    "                  'learning_rate':[0.2]}\n",
    "    abc = GridSearchCV(AdaBoostClassifier(), param_grid, cv=cv_value, return_train_score=False)\n",
    "    abc.fit(X_train, y_train)\n",
    "    y_predabc = abc.predict(X_test)\n",
    "    queue.put(y_predabc)\n",
    "    print(\"Ada Boost: \", abc.best_params_)\n",
    "    print(\"Ada Boost: \", abc.best_score_)\n",
    "\n",
    "def rfc_function(X_train, y_train):\n",
    "    print(\"4\",'RandomForestClassifier')\n",
    "    param_grid =  {'n_estimators':np.arange(1, 20),\n",
    "                  'max_depth': np.arange(3, 15),\n",
    "                  'criterion' :['gini', 'entropy']}\n",
    "    rfc = GridSearchCV(RandomForestClassifier(), param_grid, cv=cv_value, return_train_score=False)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    y_predrfc = rfc.predict(X_test)\n",
    "    queue.put(y_predrfc)\n",
    "    print(\"Random Forest: \", rfc.best_params_)\n",
    "    print(\"Random Forest: \", rfc.best_score_)                       \n",
    "\n",
    "def svc_function(X_train, y_train):\n",
    "    print(\"5\",'SVC')\n",
    "    param_grid = {'C': [0.1, 1, 10], 'gamma': [1, 0.01],'kernel': ['rbf']}\n",
    "    svc = GridSearchCV(SVC(), param_grid, cv=cv_value, return_train_score=False)\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_predsvc = svc.predict(X_test)\n",
    "    queue.put(y_predsvc)\n",
    "    print(\"SVC: \", svc.best_params_)\n",
    "    print(\"SVC: \", svc.best_score_)\n",
    "    \n",
    "def etc_function(X_train, y_train):\n",
    "    print(\"6\",'ExtraTreeClassifier')\n",
    "    param_grid =  {'n_estimators':np.arange(1, 20),\n",
    "                  'max_depth': np.arange(3, 15),\n",
    "                  'criterion' :['gini', 'entropy']}\n",
    "    etc = GridSearchCV(ExtraTreesClassifier(), param_grid, cv=cv_value, return_train_score=False)\n",
    "    etc.fit(X_train, y_train)\n",
    "    y_predetc = etc.predict(X_test)\n",
    "    queue.put(y_predetc)\n",
    "    print(\"Etc: \", etc.best_params_)\n",
    "    print(\"Etc: \", etc.best_score_)                         \n",
    "    \n",
    "def lda_function(X_train, y_train):\n",
    "    print(\"7\",'LinearDiscriminantAnalysis')\n",
    "    param_grid =  {'solver' :['svd', 'lsqr', 'eigen']}\n",
    "    lda = GridSearchCV(LinearDiscriminantAnalysis(), param_grid, cv=cv_value, return_train_score=False)\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_predlda = lda.predict(X_test)\n",
    "    queue.put(y_predlda)\n",
    "    print(\"lda: \", lda.best_params_)\n",
    "    print(\"lda: \", lda.best_score_)\n",
    "\n",
    "def qda_function(X_train, y_train):\n",
    "    print(\"8\",'QuadraticDiscriminantAnalysis')\n",
    "    param_grid =  {'reg_param':[0.1, 0.3, 0.5]}\n",
    "    qda = GridSearchCV(QuadraticDiscriminantAnalysis(), param_grid, cv=cv_value, return_train_score=False)\n",
    "    qda.fit(X_train, y_train)\n",
    "    y_predqda = qda.predict(X_test)\n",
    "    queue.put(y_predqda)\n",
    "    print(\"qda: \", qda.best_params_)\n",
    "    print(\"qda: \", qda.best_score_)\n",
    "\n",
    "def mlp_function(X_train, y_train):\n",
    "    print(\"9\",'MLPClassifier')\n",
    "    param_grid =  {'solver':['lbfgs', 'SGD', 'ADAM'],\n",
    "                  'activation': ['relu'],\n",
    "                  'hidden_layer_sizes': [50, 100, 150]}\n",
    "    mlp = GridSearchCV(MLPClassifier(), param_grid, cv=cv_value, return_train_score=False)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_predmlp = mlp.predict(X_test)\n",
    "    queue.put(y_predmlp)\n",
    "    print(\"mlp: \", mlp.best_params_)\n",
    "    print(\"mlp: \", mlp.best_score_)\n",
    "    \n",
    "def xg_function(X_train, y_train):\n",
    "    print(\"10\",'XGBoost')\n",
    "    param_grid =  {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "                  'booster':['gblinear', 'gbtree'],\n",
    "                  'learning_rate': [0.1, 0.2, 0.3], #so called `eta` value\n",
    "                  'n_estimators': [50, 500, 1000], #number of trees, change it to 1000 for better results\n",
    "                  'missing':[-999],\n",
    "                  'seed': [1337],\n",
    "    'disable_default_eval_metric': [True]}\n",
    "    xg = GridSearchCV(xgb.XGBClassifier(), param_grid, cv=cv_value, return_train_score=False)\n",
    "    xg.fit(X_train, y_train)\n",
    "    y_predxg = xg.predict(X_test)\n",
    "    queue.put(y_predxg)\n",
    "    print(\"xgb: \", xg.best_params_)\n",
    "    print(\"xgb: \", xg.best_score_)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampler Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import queue\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "NameCluster=[]\n",
    "Contatge=[]\n",
    "performances=[]\n",
    "Dades=[]\n",
    "undersamplers  = [\"ClusterCentroid\", \"ClusterRepresentative\"]\n",
    "dfprint = []\n",
    "\n",
    "queue = queue.Queue()\n",
    "\n",
    "\n",
    "f = open(\"test.csv\", \"w\")\n",
    "f.write(\"Model, Sampler, Items, Indexes\")\n",
    "f.write('\\n') \n",
    "\n",
    "\n",
    "def resample(X, y):\n",
    "    class_counts = y.value_counts().to_dict()\n",
    "    mayority_class = max(class_counts, key=class_counts.get)\n",
    "    mayority_class_index = y[y == mayority_class].index\n",
    " \n",
    "    \n",
    "    num_clusters = min(class_counts.values())\n",
    "    clusters = KMeans(n_clusters=num_clusters)\\\n",
    "                    .fit(X.loc[mayority_class_index])\\\n",
    "                    .cluster_centers_\n",
    "    print(num_clusters)\n",
    "    closest_indices = [\n",
    "        np.argmin([np.linalg.norm(c - e) for e in X.loc[mayority_class_index].values])\n",
    "        for c in clusters\n",
    "    ]\n",
    "    representatives = X.iloc[closest_indices]\n",
    "\n",
    "    y_train = y.drop(mayority_class_index)\n",
    "    X_train = X.drop(mayority_class_index)\n",
    "\n",
    "    X = pd.concat([X, representatives])\n",
    "    y = pd.concat([y, pd.Series([mayority_class for _ in range(len(representatives))])])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def resample1(X, y):\n",
    "    class_counts = y.value_counts().to_dict()\n",
    "    mayority_class = max(class_counts, key=class_counts.get)\n",
    "    mayority_class_index = y[y == mayority_class].index\n",
    "\n",
    "    num_clusters = min(class_counts.values())\n",
    "\n",
    "    clusters = KMeans(n_clusters=num_clusters)\\\n",
    "                    .fit(X.loc[mayority_class_index])\\\n",
    "                    .cluster_centers_\n",
    "    clusters = pd.DataFrame(\n",
    "                    clusters, columns=X.columns\n",
    "                )\n",
    "\n",
    "    y = y.drop(mayority_class_index)\n",
    "    X = X.drop(mayority_class_index)\n",
    "\n",
    "    X = pd.concat([X, clusters])\n",
    "    y = pd.concat([y, pd.Series([mayority_class for _ in range(len(clusters))])])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "time_start=time.time()\n",
    "\n",
    "for Undersampler in undersamplers:\n",
    "    if Undersampler == (\"ClusterCentroid\"):\n",
    "                \n",
    "            X_train, y_train = resample(X, y)\n",
    "    else:\n",
    "\n",
    "            X_train, y_train = resample1(X, y)\n",
    "        \n",
    "    cont=0\n",
    "    print(cont)\n",
    "    lock = threading.Lock()\n",
    "    thread0 = threading.Thread(target=Knn_function, args=(X_train, y_train))\n",
    "    thread1 = threading.Thread(target=gs_function, args=(X_train, y_train))\n",
    "    thread2 = threading.Thread(target=dtc_function, args=(X_train, y_train))\n",
    "    thread3 = threading.Thread(target=abc_function, args=(X_train, y_train))\n",
    "    thread4 = threading.Thread(target=rfc_function, args=(X_train, y_train))\n",
    "    thread5 = threading.Thread(target=svc_function, args=(X_train, y_train))\n",
    "    thread6 = threading.Thread(target=etc_function, args=(X_train, y_train))\n",
    "    thread7 = threading.Thread(target=lda_function, args=(X_train, y_train))\n",
    "    thread8 = threading.Thread(target=qda_function, args=(X_train, y_train))\n",
    "    thread9 = threading.Thread(target=mlp_function, args=(X_train, y_train))\n",
    "    thread10 = threading.Thread(target=xg_function, args=(X_train, y_train))\n",
    "    \n",
    "    thread0.start() \n",
    "    thread1.start() \n",
    "    thread2.start() \n",
    "    thread3.start()\n",
    "    thread4.start() \n",
    "    thread5.start() \n",
    "    thread6.start() \n",
    "    thread7.start() \n",
    "    thread8.start() \n",
    "    thread9.start() \n",
    "    thread10.start() \n",
    "   \n",
    "    thread0.join()\n",
    "    thread1.join()\n",
    "    thread2.join() \n",
    "    thread3.join()\n",
    "    thread4.join()\n",
    "    thread5.join()\n",
    "    thread6.join() \n",
    "    thread7.join()\n",
    "    thread8.join()\n",
    "    thread9.join() \n",
    "    thread10.join()\n",
    "    \n",
    "    for Model in models:\n",
    "        model = Model()\n",
    "        try:\n",
    "\n",
    "            if cont == 0:\n",
    "                y_pred = queue.get()\n",
    "            if cont == 1:\n",
    "                y_pred = queue.get()            \n",
    "            if cont == 2:\n",
    "                y_pred = queue.get() \n",
    "            if cont == 3:\n",
    "                y_pred = queue.get()\n",
    "            if cont == 4:\n",
    "                y_pred = queue.get()                       \n",
    "            if cont == 5:\n",
    "                y_pred = queue.get()\n",
    "            if cont == 6:\n",
    "                y_pred = queue.get()                      \n",
    "            if cont == 7:\n",
    "                y_pred = queue.get()      \n",
    "            if cont == 8:\n",
    "                y_pred = queue.get()\n",
    "            if cont == 9:\n",
    "                y_pred = queue.get()\n",
    "            if cont == 10:\n",
    "                y_pred = queue.get()\n",
    "  \n",
    "        except Exception as e:\n",
    "              print(e)\n",
    "       \n",
    "    \n",
    "        print(model)\n",
    "        NameCluster.append(model)\n",
    "        NameCluster.append(Undersampler)\n",
    "        NameCluster.append(sum(y_pred))\n",
    "        dfprint.extend([model, Undersampler, sum(y_pred)])\n",
    "        for n in range(len(y_pred)):\n",
    "            if (y_pred[n]==True):\n",
    "                NameCluster2=[X_test.index[n]]\n",
    "                Contatge.extend(NameCluster2)\n",
    "                NameCluster.extend([NameCluster2])\n",
    "                dfprint.extend([NameCluster2])       \n",
    "        f.write(str(dfprint)+'\\n')        \n",
    "        dfprint.clear()\n",
    "        Dades.append([sum(y_pred == True), Undersampler, model])\n",
    "        cont=(cont + 1)\n",
    "f.write('\\n')\n",
    "end_time=time.time()\n",
    "print(\"total time: \", (end_time - time_start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampler Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bar_plot=[]\n",
    "performances=[]\n",
    "Dades=[]\n",
    "\n",
    "time_start=time.time()\n",
    "\n",
    "for Oversampler in oversamplers:\n",
    "    if Oversampler == (SMOTE) or Oversampler == (ADASYN):\n",
    "            X_train, y_train = Oversampler(n_jobs=-1).fit_resample(X, y)\n",
    "    elif Oversampler == (SPIDER):     \n",
    "            X_train, y_train = Oversampler(n_jobs=-1).sample(X.values, y.values)\n",
    "    else:\n",
    "            X_train, y_train = Oversampler().sample(X.values, y.values)\n",
    "\n",
    "    cont=0\n",
    "    print(cont)\n",
    "    lock = threading.Lock()\n",
    "    thread0 = threading.Thread(target=Knn_function, args=(X_train, y_train))\n",
    "    thread1 = threading.Thread(target=gs_function, args=(X_train, y_train))\n",
    "    thread2 = threading.Thread(target=dtc_function, args=(X_train, y_train))\n",
    "    thread3 = threading.Thread(target=abc_function, args=(X_train, y_train))\n",
    "    thread4 = threading.Thread(target=rfc_function, args=(X_train, y_train))\n",
    "    thread5 = threading.Thread(target=svc_function, args=(X_train, y_train))\n",
    "    thread6 = threading.Thread(target=etc_function, args=(X_train, y_train))\n",
    "    thread7 = threading.Thread(target=lda_function, args=(X_train, y_train))\n",
    "    thread8 = threading.Thread(target=qda_function, args=(X_train, y_train))\n",
    "    thread9 = threading.Thread(target=mlp_function, args=(X_train, y_train))\n",
    "    thread10 = threading.Thread(target=xg_function, args=(X_train, y_train))\n",
    "   \n",
    "    thread0.start() \n",
    "    thread1.start() \n",
    "    thread2.start() \n",
    "    thread3.start()\n",
    "    thread4.start() \n",
    "    thread5.start() \n",
    "    thread6.start() \n",
    "    thread7.start() \n",
    "    thread8.start() \n",
    "    thread9.start() \n",
    "    thread10.start() \n",
    "    \n",
    "    thread0.join()\n",
    "    thread1.join()\n",
    "    thread2.join() \n",
    "    thread3.join()\n",
    "    thread4.join()\n",
    "    thread5.join()\n",
    "    thread6.join() \n",
    "    thread7.join()\n",
    "    thread8.join()\n",
    "    thread9.join() \n",
    "    thread10.join()\n",
    "    \n",
    "    for Model in models:\n",
    "        model = Model()\n",
    "        try:\n",
    "\n",
    "            if cont == 0:\n",
    "                y_pred = queue.get()\n",
    "            if cont == 1:\n",
    "                y_pred = queue.get()          \n",
    "            if cont == 2:\n",
    "                y_pred = queue.get()\n",
    "            if cont == 3:\n",
    "                y_pred = queue.get()\n",
    "            if cont == 4:\n",
    "                y_pred = queue.get()                       \n",
    "            if cont == 5:\n",
    "                y_pred = queue.get()\n",
    "            if cont == 6:\n",
    "                y_pred = queue.get()                     \n",
    "            if cont == 7:\n",
    "                y_pred = queue.get()    \n",
    "            if cont == 8:\n",
    "                y_pred = queue.get()\n",
    "            if cont == 9:\n",
    "                y_pred = queue.get()\n",
    "            if cont == 10:\n",
    "                y_pred = queue.get()\n",
    "  \n",
    "        except Exception as e:\n",
    "                print(e)\n",
    "        NameCluster.append(model)\n",
    "        NameCluster.append(Oversampler)\n",
    "        NameCluster.append(sum(y_pred))\n",
    "        dfprint.extend([model, Oversampler, sum(y_pred)])\n",
    "        for n in range(len(y_pred)):\n",
    "            if (y_pred[n]==True):\n",
    "                NameCluster2=[X_test.index[n]]\n",
    "                Contatge.extend(NameCluster2)\n",
    "                NameCluster.extend([NameCluster2])\n",
    "                dfprint.extend([NameCluster2]) \n",
    "        f.write(str(dfprint)+'\\n')        \n",
    "        dfprint.clear()\n",
    "        Dades.append([sum(y_pred == True), Oversampler, model])\n",
    "        cont=(cont + 1)\n",
    "        print(cont, model)\n",
    "    f.write('\\n') \n",
    "f.close()\n",
    "end_time=time.time()\n",
    "print(\"total time: \", (end_time - time_start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRINT RESULTS\n",
    "in excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di= pd.DataFrame(NameCluster).T\n",
    "de = pd.DataFrame(Dades).T\n",
    "Contatge2=set(Contatge)\n",
    "key=[]\n",
    "value=[]\n",
    "Percentage=[]\n",
    "key=list(Contatge2)\n",
    "for number in Contatge2:\n",
    "    value.append(Contatge.count(number))\n",
    "for number in Contatge2:\n",
    "    Percentage.append((key[value.index(max(value))],(max(value)*100)/((len(undersamplers)+len(oversamplers))*len(models))))\n",
    "    key.pop(value.index(max(value)))\n",
    "    value.pop(value.index(max(value)))\n",
    "df = pd.DataFrame(Percentage, columns=['Name_Cluster','% Models'])\n",
    "with pd.ExcelWriter(\"Resultats.xlsx\") as writer:\n",
    "    df.to_excel(writer, sheet_name='Percentage')\n",
    "    de.to_excel(writer, sheet_name='Models')\n",
    "    \n",
    "    di.to_excel(writer, sheet_name='Indexes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
